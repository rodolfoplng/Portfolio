{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cccd0f6a",
   "metadata": {},
   "source": [
    "# Parquet ETL with PySpark\n",
    "**Last updated:** 2025-11-13\n",
    "\n",
    "This notebook demonstrates a minimal ETL using **PySpark** and **Parquet**:\n",
    "\n",
    "1. Read a Parquet file (or generate a sample if missing)  \n",
    "2. Inspect schema and preview records  \n",
    "3. Filter and derive a new column  \n",
    "4. Save results to Parquet (partitioned) and CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc97413-2ba2-47bd-a7eb-6fa26eddcabc",
   "metadata": {},
   "source": [
    "> **⚠️ Important Note (Prerequisites)**  \n",
    "> To run this notebook successfully, the following components must be properly installed and configured on your system:  \n",
    ">\n",
    "> 1. **Java JDK (17 recommended)**  \n",
    ">    - Modern versions of Apache Spark (3.4 and newer) fully support and often recommend **Java 17 or later**.  \n",
    ">    - Make sure the `JAVA_HOME` environment variable points to the installed JDK.\n",
    ">   \n",
    "> 2. **Apache Spark**  \n",
    ">    - Use a pre-built distribution such as *spark-3.x-bin-hadoop3*.  \n",
    ">    - Set the `SPARK_HOME` environment variable and include `SPARK_HOME\\bin` in your system `PATH`.  \n",
    ">  \n",
    "> 3. **WinUtils (Windows only)**  \n",
    ">    - Windows requires `winutils.exe` inside the folder `SPARK_HOME\\bin`.  \n",
    ">    - This is needed for local Hadoop compatibility and file permission handling.  \n",
    ">  \n",
    "> 4. **PySpark** installed in your Python environment  \n",
    ">    ```bash\n",
    ">    pip install pyspark\n",
    ">    ```\n",
    ">  \n",
    "> 5. (Optional) **PyArrow** for improved Parquet performance  \n",
    ">    ```bash\n",
    ">    pip install pyarrow\n",
    ">    ```\n",
    ">  \n",
    "> If any of these components are missing or incorrectly configured, the Spark session may fail to start or may be unable to read Parquet files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bee50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "#%pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bfe067-bba0-4ed4-84c6-46428e29415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.16.8-hotspot\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.7-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\spark\\spark-3.5.7-bin-hadoop3\\hadoop\"\n",
    "os.environ[\"PATH\"] = os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"SPARK_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22943f29-1f3e-4d99-9f98-3405ef837987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2659632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2DSRRA8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ParquetETL-Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26a75d1efd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, functions as f\n",
    "\n",
    "# --- Paths ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IN_PATH = str(DATA_DIR / \"orders.parquet\")\n",
    "OUT_PARQUET = str(DATA_DIR / \"orders_clean_spark.parquet\")\n",
    "OUT_CSV_DIR = str(DATA_DIR / \"orders_clean_spark_csv\")\n",
    "\n",
    "# --- Create Spark session ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName(\"ParquetETL-Spark\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98eaaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset written to data\\orders.parquet\n"
     ]
    }
   ],
   "source": [
    "# If input doesn't exist, create a small sample with pandas and write Parquet\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "if not Path(IN_PATH).exists():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        engine = \"pyarrow\"\n",
    "    except Exception:\n",
    "        engine = \"fastparquet\"\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    n = 1000\n",
    "    df_sample = pd.DataFrame({\n",
    "        \"order_id\": range(1, n+1),\n",
    "        \"unit_price\": rng.uniform(5, 500, size=n).round(2),\n",
    "        \"quantity\": rng.integers(1, 10, size=n),\n",
    "        \"order_date\": pd.date_range(\"2024-01-01\", periods=n, freq=\"H\"),\n",
    "        \"country\": rng.choice([\"Brazil\",\"USA\",\"Spain\",\"Germany\"], size=n, p=[0.4,0.3,0.2,0.1])\n",
    "    })\n",
    "    # Convert timestamp ns to us (Spark compatible)\n",
    "    df_sample[\"order_date\"] = df_sample[\"order_date\"].astype(\"datetime64[us]\")\n",
    "    \n",
    "    df_sample.to_parquet(IN_PATH, engine=engine, compression=\"snappy\", index=False)\n",
    "    print(f\"Sample dataset written to {IN_PATH}\")\n",
    "else:\n",
    "    print(f\"Found existing dataset at {IN_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab38cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------------------+-------+\n",
      "|order_id|unit_price|quantity|order_date         |country|\n",
      "+--------+----------+--------+-------------------+-------+\n",
      "|1       |388.11    |8       |2024-01-01 00:00:00|USA    |\n",
      "|2       |222.24    |1       |2024-01-01 01:00:00|Germany|\n",
      "|3       |430.01    |4       |2024-01-01 02:00:00|Brazil |\n",
      "|4       |350.2     |5       |2024-01-01 03:00:00|USA    |\n",
      "|5       |51.62     |8       |2024-01-01 04:00:00|Spain  |\n",
      "+--------+----------+--------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- order_date: timestamp_ntz (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Read Parquet (Spark will push down column selection and filters later)\n",
    "df = spark.read.parquet(IN_PATH)\n",
    "\n",
    "# Preview & schema\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a31acdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------------------+-------+-----------+\n",
      "|order_id|unit_price|quantity|order_date         |country|total_value|\n",
      "+--------+----------+--------+-------------------+-------+-----------+\n",
      "|3       |430.01    |4       |2024-01-01 02:00:00|Brazil |1720.04    |\n",
      "|9       |68.42     |8       |2024-01-01 08:00:00|Brazil |547.36     |\n",
      "|11      |188.55    |5       |2024-01-01 10:00:00|Brazil |942.75     |\n",
      "|12      |463.75    |4       |2024-01-01 11:00:00|Brazil |1855.0     |\n",
      "|16      |117.48    |3       |2024-01-01 15:00:00|Brazil |352.44     |\n",
      "+--------+----------+--------+-------------------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Processing: filters and derived column\n",
    "# - Keep country == 'Brazil'\n",
    "# - Keep quantity >= 2\n",
    "# - total_value = unit_price * quantity\n",
    "\n",
    "if \"country\" in df.columns:\n",
    "    df = df.filter(f.col(\"country\") == f.lit(\"Brazil\"))\n",
    "\n",
    "if \"quantity\" in df.columns:\n",
    "    df = df.filter(f.col(\"quantity\") >= f.lit(2))\n",
    "\n",
    "if \"unit_price\" in df.columns and \"quantity\" in df.columns:\n",
    "    df = df.withColumn(\n",
    "        \"total_value\",\n",
    "        (f.col(\"unit_price\").cast(\"double\") * f.col(\"quantity\").cast(\"double\")).cast(\"double\")\n",
    "    )\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fce7147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|order_date_only|daily_sales       |\n",
      "+---------------+------------------+\n",
      "|2024-01-01     |12167.53          |\n",
      "|2024-01-02     |20996.5           |\n",
      "|2024-01-03     |13488.109999999999|\n",
      "|2024-01-04     |5912.37           |\n",
      "|2024-01-05     |20238.03          |\n",
      "+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional: aggregation (daily sales)\n",
    "if \"order_date\" in df.columns and \"total_value\" in df.columns:\n",
    "    df = df.withColumn(\"order_ts\", f.to_timestamp(\"order_date\"))\n",
    "    daily = (df\n",
    "             .withColumn(\"order_date_only\", f.to_date(\"order_ts\"))\n",
    "             .groupBy(\"order_date_only\")\n",
    "             .agg(f.sum(\"total_value\").alias(\"daily_sales\"))\n",
    "             .orderBy(\"order_date_only\"))\n",
    "    daily.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95bebe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote (Parquet folder): data\\orders_clean_spark.parquet\n",
      "Wrote (CSV folder): data\\orders_clean_spark_csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Save results\n",
    "# Parquet (partitioned by order_year if order_date exists)\n",
    "if \"order_date\" in df.columns:\n",
    "    df = df.withColumn(\"order_ts\", f.to_timestamp(\"order_date\"))\n",
    "    df = df.withColumn(\"order_year\", f.year(\"order_ts\"))\n",
    "    (df.write\n",
    "       .mode(\"overwrite\")\n",
    "       .partitionBy(\"order_year\")\n",
    "       .parquet(OUT_PARQUET))\n",
    "else:\n",
    "    df.write.mode(\"overwrite\").parquet(OUT_PARQUET)\n",
    "\n",
    "# CSV (coalesce to a single file for demo readability)\n",
    "(df.coalesce(1)\n",
    "   .write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"header\", \"true\")\n",
    "   .csv(OUT_CSV_DIR))\n",
    "\n",
    "print(\"Wrote (Parquet folder):\", OUT_PARQUET)\n",
    "print(\"Wrote (CSV folder):\", OUT_CSV_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5b1d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the session (free resources)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
